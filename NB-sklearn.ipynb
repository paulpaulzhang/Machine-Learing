{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba as jb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_review(data):\n",
    "    \"\"\"将样本进行分词处理\"\"\"\n",
    "    result = []\n",
    "    stopWord = [' ', ',', '?', '.','-','“','”','/','’', 'is']\n",
    "    for d in data:\n",
    "        result.append(list(filter(lambda s: s and s not in stopWord, jb.lcut(d))))\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_vocab_list(dataSet):\n",
    "    \"\"\"将所有词条集合传入，得到一个所有不重复词条的集合字典\"\"\"\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)\n",
    "\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    \"\"\"将词条集合转换为词条向量\"\"\"\n",
    "    returnVec = np.zeros(len(vocabList))\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "    return returnVec\n",
    "\n",
    "def load_train_dataset(path):\n",
    "    dataset = pd.read_csv(path, lineterminator='\\n') \n",
    "    data = cut_review(dataset.values[:, 1])\n",
    "\n",
    "    target = dataset.values[:, 2].copy()\n",
    "    target[dataset.values[:, 2] == 'Negative'] = 0\n",
    "    target[dataset.values[:, 2] == 'Positive'] = 1\n",
    "\n",
    "    return data, np.array(target, dtype='int')\n",
    "\n",
    "def load_test_dataset(path):\n",
    "    dataset = pd.read_csv(path, lineterminator='\\n')\n",
    "    data_id = dataset.values[:, 0]\n",
    "    data = cut_review(dataset.values[:, 1])\n",
    "    return data_id, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\zlm31\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.650 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "data, target = load_train_dataset('datasets/train.csv')\n",
    "data_id, dataf = load_test_dataset('datasets/test.csv')\n",
    "vocabList = create_vocab_list(data) # 词条字典\n",
    "trainMatrix = [] # 建立词条向量\n",
    "for review in data:\n",
    "    trainMatrix.append(setOfWords2Vec(vocabList, review))\n",
    "\n",
    "testMatrix = [] # 建立词条向量\n",
    "for review in dataf:\n",
    "    testMatrix.append(setOfWords2Vec(vocabList, review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(trainMatrix, target, random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf = naive_bayes.BernoulliNB()\n",
    "nb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.00373424e-11, 1.00000000e+00],\n",
       "       [3.72440418e-05, 9.99962756e-01],\n",
       "       [3.96813791e-02, 9.60318621e-01],\n",
       "       ...,\n",
       "       [9.58071389e-01, 4.19286115e-02],\n",
       "       [6.56808180e-01, 3.43191820e-01],\n",
       "       [9.89239054e-01, 1.07609455e-02]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7553729456384324"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = nb_clf.predict_proba(testMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID          Pred\n",
      "0        1  9.835102e-01\n",
      "1        2  7.727091e-01\n",
      "2        3  2.261073e-01\n",
      "3        4  9.991258e-01\n",
      "4        5  1.397580e-02\n",
      "5        6  9.999639e-01\n",
      "6        7  9.999886e-01\n",
      "7        8  7.811839e-01\n",
      "8        9  7.410139e-01\n",
      "9       10  3.883135e-01\n",
      "10      11  9.991434e-01\n",
      "11      12  1.180573e-01\n",
      "12      13  8.630645e-01\n",
      "13      14  9.903312e-01\n",
      "14      15  9.002625e-01\n",
      "15      16  7.104020e-02\n",
      "16      17  6.783145e-03\n",
      "17      18  6.109343e-01\n",
      "18      19  9.999242e-01\n",
      "19      20  9.999997e-01\n",
      "20      21  6.868052e-01\n",
      "21      22  1.599020e-05\n",
      "22      23  4.261495e-01\n",
      "23      24  9.990261e-01\n",
      "24      25  1.873580e-01\n",
      "25      26  2.778957e-08\n",
      "26      27  6.051532e-02\n",
      "27      28  3.833371e-01\n",
      "28      29  3.339785e-01\n",
      "29      30  9.508784e-01\n",
      "...    ...           ...\n",
      "2682  2683  1.277458e-01\n",
      "2683  2684  9.999520e-01\n",
      "2684  2685  9.965837e-01\n",
      "2685  2686  4.490566e-02\n",
      "2686  2687  9.997511e-01\n",
      "2687  2688  9.997883e-01\n",
      "2688  2689  9.999995e-01\n",
      "2689  2690  1.000000e+00\n",
      "2690  2691  6.679302e-01\n",
      "2691  2692  8.867785e-01\n",
      "2692  2693  9.986369e-01\n",
      "2693  2694  9.901258e-01\n",
      "2694  2695  9.947656e-01\n",
      "2695  2696  6.604457e-03\n",
      "2696  2697  9.965713e-01\n",
      "2697  2698  9.999962e-01\n",
      "2698  2699  5.692044e-02\n",
      "2699  2700  9.997142e-01\n",
      "2700  2701  2.316543e-02\n",
      "2701  2702  9.996484e-01\n",
      "2702  2703  9.995734e-01\n",
      "2703  2704  9.999264e-01\n",
      "2704  2705  6.206542e-01\n",
      "2705  2706  5.283884e-01\n",
      "2706  2707  8.869299e-02\n",
      "2707  2708  9.934729e-01\n",
      "2708  2709  9.999818e-01\n",
      "2709  2710  6.432926e-01\n",
      "2710  2711  2.621952e-02\n",
      "2711  2712  2.391783e-07\n",
      "\n",
      "[2712 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "res = {'ID': data_id, 'Pred': result[:, 1]}\n",
    "df = pd.DataFrame(res, columns=['ID', 'Pred'])\n",
    "print(df)\n",
    "df.to_csv('datasets/result.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordfrequency(vocabList, inputSet):\n",
    "    \"\"\"统计词频\"\"\"\n",
    "    wordFreDict = {}\n",
    "    for words in inputSet:\n",
    "        for word in words:\n",
    "            if word in vocabList:\n",
    "                if word not in wordFreDict.keys():\n",
    "                    wordFreDict[word] = 0\n",
    "                wordFreDict[word] += 1\n",
    "    words = dict(sorted(wordFreDict.items(),key=lambda x: x[1], reverse=True))\n",
    "    return list(words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDict = wordfrequency(vocabList, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ki',\n",
       " 'ke',\n",
       " 'mein',\n",
       " 'hai',\n",
       " 'ko',\n",
       " 'aur',\n",
       " 'ka',\n",
       " 'se',\n",
       " 'k',\n",
       " 'ne',\n",
       " 'bhi',\n",
       " 'to',\n",
       " 'ho',\n",
       " 'hain',\n",
       " 'par',\n",
       " 'kar',\n",
       " 'e',\n",
       " 'na',\n",
       " 'nahi',\n",
       " 'tha',\n",
       " 'Allah',\n",
       " 'aik',\n",
       " 'ye',\n",
       " 'ha',\n",
       " 'or',\n",
       " 'hi',\n",
       " 'in',\n",
       " 'wo',\n",
       " 'he',\n",
       " 'b',\n",
       " 'K',\n",
       " '😂',\n",
       " '...',\n",
       " 'kiya',\n",
       " 'jo',\n",
       " 'main',\n",
       " 'kay',\n",
       " ':',\n",
       " '..',\n",
       " 'Pakistan',\n",
       " 'ap',\n",
       " 'koi',\n",
       " 'Ki',\n",
       " 'un',\n",
       " 'hy',\n",
       " 'thi',\n",
       " 'liye',\n",
       " 'the',\n",
       " 'Ka',\n",
       " 'sath',\n",
       " 'Me',\n",
       " 'Khan',\n",
       " 'h',\n",
       " 'apni',\n",
       " 'yeh',\n",
       " 'me',\n",
       " 'ga',\n",
       " 'gaya',\n",
       " 'bad',\n",
       " 'kr',\n",
       " 'bohat',\n",
       " 'Se',\n",
       " 'acha',\n",
       " 'us',\n",
       " 'apne',\n",
       " 'sab',\n",
       " 'say',\n",
       " 'kuch',\n",
       " 'tak',\n",
       " 'tu',\n",
       " 'aap',\n",
       " 'o',\n",
       " 'Ko',\n",
       " 'kia',\n",
       " 'nhi',\n",
       " ')',\n",
       " 'hum',\n",
       " 'ya',\n",
       " '!',\n",
       " 'ab',\n",
       " 'gai',\n",
       " 'raha',\n",
       " 'per',\n",
       " 'jis',\n",
       " 'nai',\n",
       " 'Is',\n",
       " 'diya',\n",
       " 'kaam',\n",
       " 'bht',\n",
       " 'sy',\n",
       " 'baat',\n",
       " 'nay',\n",
       " '....',\n",
       " 'kisi',\n",
       " 'achi',\n",
       " 'lekin',\n",
       " 'ni',\n",
       " 'hasil',\n",
       " 'ALLAH',\n",
       " 'jab',\n",
       " 'rahe',\n",
       " 'hota',\n",
       " 'dua',\n",
       " 'pak',\n",
       " 'rahi',\n",
       " '(',\n",
       " 'pe',\n",
       " 'leye',\n",
       " 'Or',\n",
       " 'tou',\n",
       " 'khud',\n",
       " 'karne',\n",
       " '2',\n",
       " 'sirf',\n",
       " 'kam',\n",
       " 'film',\n",
       " 'ker',\n",
       " 'bhai',\n",
       " 'ma',\n",
       " 'de',\n",
       " 'meri',\n",
       " 'Ye',\n",
       " 'di',\n",
       " 'mai',\n",
       " 'Hai',\n",
       " 'iss',\n",
       " 'log',\n",
       " 'hua',\n",
       " 'dil',\n",
       " 'saal',\n",
       " 'hone',\n",
       " 'gaye',\n",
       " 'Main',\n",
       " 'woh',\n",
       " 'kya',\n",
       " 'wale',\n",
       " 'key',\n",
       " 'do',\n",
       " 'ata',\n",
       " '3',\n",
       " 'har',\n",
       " 'sey',\n",
       " '\\n',\n",
       " 'din',\n",
       " 'Ali',\n",
       " 'hay',\n",
       " 'wali',\n",
       " 'bat',\n",
       " 'phir',\n",
       " 'Jo',\n",
       " 'a',\n",
       " 'tum',\n",
       " 'good',\n",
       " 'and',\n",
       " 'unhon',\n",
       " 'Aur',\n",
       " 'hn',\n",
       " 'apna',\n",
       " 'D',\n",
       " 'ja',\n",
       " 'inho',\n",
       " 'jata',\n",
       " '�',\n",
       " 'hun',\n",
       " 'hoti',\n",
       " 'kaha',\n",
       " '1',\n",
       " 'zindagi',\n",
       " 'quality',\n",
       " 'g',\n",
       " 'Ameen',\n",
       " 'mujhe',\n",
       " 'dy',\n",
       " 'logon',\n",
       " 'Ne',\n",
       " 'karna',\n",
       " 'ameen',\n",
       " 'Imran',\n",
       " 'mulk',\n",
       " 'mera',\n",
       " 'agar',\n",
       " 'pr',\n",
       " 'ky',\n",
       " 'you',\n",
       " 'team',\n",
       " 'ny',\n",
       " 'hui',\n",
       " 'they',\n",
       " 'Wo',\n",
       " 'hue',\n",
       " 'naam',\n",
       " \"'\",\n",
       " 'shadi',\n",
       " 'sb',\n",
       " 'bilkul',\n",
       " 'Hussain',\n",
       " 'm',\n",
       " 'hon',\n",
       " 'Karachi',\n",
       " 'Per',\n",
       " 'mere',\n",
       " 'Kar',\n",
       " 'aa',\n",
       " 'aunty',\n",
       " 'taraf',\n",
       " 'EEE',\n",
       " 'deya',\n",
       " 'gi',\n",
       " 'le',\n",
       " 'liya',\n",
       " 'bana',\n",
       " 'waqt',\n",
       " 'duniya',\n",
       " '\"',\n",
       " 'karte',\n",
       " 'dekh',\n",
       " 'Lahore',\n",
       " 'mien',\n",
       " 'Ya',\n",
       " 'of',\n",
       " '10',\n",
       " 'Kia',\n",
       " 'ban',\n",
       " 'baad',\n",
       " 'sa',\n",
       " 'thay',\n",
       " 'tarah',\n",
       " 'mil',\n",
       " 'Hy',\n",
       " 'laga',\n",
       " 'bar',\n",
       " 'wala',\n",
       " 'app',\n",
       " '#',\n",
       " 'sakta',\n",
       " 'day',\n",
       " 'aya',\n",
       " 'drama',\n",
       " 'ek',\n",
       " 'inhe',\n",
       " 'Pakistani',\n",
       " 'sub',\n",
       " 'bari',\n",
       " 'kiye',\n",
       " 'gya',\n",
       " '.....',\n",
       " 'In',\n",
       " 'karo',\n",
       " 'khan',\n",
       " 'allah',\n",
       " 'hoi',\n",
       " 'Nahi',\n",
       " 'nae',\n",
       " 'lakin',\n",
       " 'band',\n",
       " 'itna',\n",
       " 'I',\n",
       " 's',\n",
       " 'Jab',\n",
       " 'chor',\n",
       " 'Ap',\n",
       " 'magar',\n",
       " 'Pr',\n",
       " 'p',\n",
       " 'jaye',\n",
       " 'Azam',\n",
       " 'hen',\n",
       " 'nazar',\n",
       " 'behtar',\n",
       " 'logo',\n",
       " 'karta',\n",
       " 'hona',\n",
       " 'hoon',\n",
       " 'Yeh',\n",
       " 'Nawaz',\n",
       " 'kabhi',\n",
       " 'rha',\n",
       " 'pasand',\n",
       " 'toh',\n",
       " 'cricket',\n",
       " 'py',\n",
       " 'tamam',\n",
       " 'Jis',\n",
       " 'pata',\n",
       " 'ghar',\n",
       " 'sahi',\n",
       " 'ta',\n",
       " 'kafi',\n",
       " 'plz',\n",
       " '5',\n",
       " 'insan',\n",
       " 'Na',\n",
       " 'be',\n",
       " 'Ho',\n",
       " 'baras',\n",
       " 'Good',\n",
       " 'time',\n",
       " 'Iqbal',\n",
       " 'shamil',\n",
       " 'gae',\n",
       " 'awam',\n",
       " 'pesh',\n",
       " 'mila',\n",
       " 'Meri',\n",
       " 'jati',\n",
       " 'Sharif',\n",
       " 'aaj',\n",
       " 'apny',\n",
       " 'but',\n",
       " 'use',\n",
       " 'kyun',\n",
       " '€',\n",
       " 'u',\n",
       " 'pehle',\n",
       " 'lanat',\n",
       " 'khush',\n",
       " 'krna',\n",
       " 'lag',\n",
       " 'lo',\n",
       " 'dono',\n",
       " 'bs',\n",
       " 'tor',\n",
       " 'my',\n",
       " 'pas',\n",
       " 'boht',\n",
       " 'madad',\n",
       " 'Police',\n",
       " 'yad',\n",
       " 'kro',\n",
       " 'Hahaha',\n",
       " 'dia',\n",
       " '🤣',\n",
       " 'khilaf',\n",
       " 'bi',\n",
       " 'wajah',\n",
       " 'ul',\n",
       " 'aj',\n",
       " 'Mein',\n",
       " 'Lanat',\n",
       " 'howa',\n",
       " 'walon',\n",
       " 'khawateen',\n",
       " 'chahiye',\n",
       " 'phr',\n",
       " 'pakistan',\n",
       " 'shuru',\n",
       " 'bara',\n",
       " 'open',\n",
       " 'jeet',\n",
       " 'show',\n",
       " 'tareen',\n",
       " 'Ab',\n",
       " 'nh',\n",
       " 'Firing',\n",
       " 'Aap',\n",
       " 'kai',\n",
       " 'han',\n",
       " 'may',\n",
       " 'Agar',\n",
       " 'ge',\n",
       " 'bhee',\n",
       " 'itni',\n",
       " 'jin',\n",
       " 'video',\n",
       " 'police',\n",
       " 'Koi',\n",
       " 'Sindh',\n",
       " 'Urdu',\n",
       " 'Jahangir',\n",
       " 'khatam',\n",
       " 'cheez',\n",
       " 'Bhai',\n",
       " 'umar',\n",
       " 'Dua',\n",
       " 'To',\n",
       " 'i',\n",
       " 'taleem',\n",
       " 'rahay',\n",
       " 'Iss',\n",
       " 'waqat',\n",
       " 'bare',\n",
       " 'yaad',\n",
       " '4',\n",
       " 'kha',\n",
       " 'Zakhmi',\n",
       " 'Tum',\n",
       " 'Hazrat',\n",
       " 'li',\n",
       " 'gy',\n",
       " 'Aik',\n",
       " 'sabit',\n",
       " 'salamat',\n",
       " 'dain',\n",
       " 'Liye',\n",
       " 'sir',\n",
       " 'hm',\n",
       " 'final',\n",
       " 'azeem',\n",
       " 'Hashim',\n",
       " 'wahan',\n",
       " 'ziyada',\n",
       " 'si',\n",
       " 'qarar',\n",
       " 'so',\n",
       " 'dete',\n",
       " 'kis',\n",
       " 'chal',\n",
       " 'nahe',\n",
       " 'Acha',\n",
       " 'umer',\n",
       " 'kry',\n",
       " 'maza',\n",
       " 'hu',\n",
       " 'Abdul',\n",
       " 'paida',\n",
       " 'British',\n",
       " 'pta',\n",
       " 'one',\n",
       " 'rakhy',\n",
       " 'hukumat',\n",
       " 'kare',\n",
       " 'kamyab',\n",
       " 'aisa',\n",
       " 'bhot',\n",
       " 'dekha',\n",
       " 'awaz',\n",
       " 'khidmat',\n",
       " 'kon',\n",
       " 'hote',\n",
       " 'rhi',\n",
       " 'Un',\n",
       " 'es',\n",
       " 'hath',\n",
       " 'Par',\n",
       " 'hamesha',\n",
       " 'ziyadah',\n",
       " 'Hain',\n",
       " 'dauran',\n",
       " 'Afrad',\n",
       " 'farma',\n",
       " 'acting',\n",
       " 'yar',\n",
       " 'all',\n",
       " 'Bht',\n",
       " 'walid',\n",
       " 'bus',\n",
       " 'Punjab',\n",
       " 'lye',\n",
       " 'Bhi',\n",
       " 'Bilkul',\n",
       " 'abhi',\n",
       " 'Squash',\n",
       " 'MQM',\n",
       " 'khayal',\n",
       " 'jae',\n",
       " 'films',\n",
       " 'dost',\n",
       " 'Haq',\n",
       " '8',\n",
       " 'amal',\n",
       " 'aam',\n",
       " 'apnay',\n",
       " 'runs',\n",
       " 'apko',\n",
       " 'Janbahaq',\n",
       " 'Us',\n",
       " 'PTI',\n",
       " 'Mere',\n",
       " 'q',\n",
       " 'kary',\n",
       " 'waly',\n",
       " 'mjhe',\n",
       " 'tw',\n",
       " 'hoga',\n",
       " 'Shaheed',\n",
       " 'behtareen',\n",
       " 'rakh',\n",
       " '6',\n",
       " 'mar',\n",
       " 'n',\n",
       " 'keya',\n",
       " 'chuke',\n",
       " 'dor',\n",
       " 'walo',\n",
       " 'jate',\n",
       " 'her',\n",
       " 'yehi',\n",
       " 'aise',\n",
       " 'karti',\n",
       " 'test',\n",
       " 'Ahmed',\n",
       " 'thy',\n",
       " 'family',\n",
       " 'hein',\n",
       " 'taur',\n",
       " 'tm',\n",
       " 'Ghar',\n",
       " 'unki',\n",
       " 'mat',\n",
       " 'Eid',\n",
       " 'isi',\n",
       " 'likha',\n",
       " 'bachon',\n",
       " 'nawaz',\n",
       " 'kirdar',\n",
       " 'record',\n",
       " 'haq',\n",
       " 'Hum',\n",
       " 'jaise',\n",
       " 'Sir',\n",
       " 'match',\n",
       " 'it',\n",
       " 'America',\n",
       " 'this',\n",
       " 'aye',\n",
       " 'kamyabi',\n",
       " 'jan',\n",
       " 'Edhi',\n",
       " 'P',\n",
       " 'da',\n",
       " 'krty',\n",
       " 'raat',\n",
       " 'walay',\n",
       " 'no',\n",
       " 'rakha',\n",
       " 'bol',\n",
       " 'product',\n",
       " 'jahan',\n",
       " 'England',\n",
       " 'khas',\n",
       " 'bura',\n",
       " 'Mubarak',\n",
       " 'ney',\n",
       " 'Pak',\n",
       " 'hawale',\n",
       " 'lga',\n",
       " 'lia',\n",
       " 'eid',\n",
       " 'Aaj',\n",
       " 'samne',\n",
       " 'Wazir',\n",
       " 'unhen',\n",
       " 'sahab',\n",
       " 'qaim',\n",
       " 'iska',\n",
       " 'dena',\n",
       " 'hissa',\n",
       " 'yahan',\n",
       " 'Anwar',\n",
       " 'league',\n",
       " 'hal',\n",
       " 'rahy',\n",
       " 'leader',\n",
       " 'keh',\n",
       " 'bal',\n",
       " 'price',\n",
       " 'jane',\n",
       " 'jawab',\n",
       " 'chand',\n",
       " 'huye',\n",
       " 'pehli',\n",
       " 'nahein',\n",
       " 'ham',\n",
       " 'Mulk',\n",
       " 'lambi',\n",
       " 'saath',\n",
       " 'apki',\n",
       " 'size',\n",
       " 'nikal',\n",
       " 'koshish',\n",
       " 'party',\n",
       " 'Bohat',\n",
       " 'lagta',\n",
       " 'balke',\n",
       " 'leya',\n",
       " 'vote',\n",
       " 'for',\n",
       " 'karay',\n",
       " 'love',\n",
       " 'jahil',\n",
       " 'Muhammad',\n",
       " 'huwa',\n",
       " 'iski',\n",
       " 'daraz',\n",
       " 'khair',\n",
       " 'mushkil',\n",
       " 'tarha',\n",
       " 'Bharat',\n",
       " 'jb',\n",
       " '😁',\n",
       " 'rhy',\n",
       " 'Kr',\n",
       " 'had',\n",
       " 'Miandad',\n",
       " 'Inayat',\n",
       " 'sakti',\n",
       " 'our',\n",
       " '…',\n",
       " 'halat',\n",
       " 'jese',\n",
       " 'jaan',\n",
       " 'siyasi',\n",
       " '20',\n",
       " 'Sirf',\n",
       " 'kal',\n",
       " 'khandan',\n",
       " 'aaya',\n",
       " 'Rangers',\n",
       " 'Thank',\n",
       " 'bhe',\n",
       " 'ur',\n",
       " 'on',\n",
       " 'Mera',\n",
       " 'that',\n",
       " 'lay',\n",
       " 'masla',\n",
       " 'jail',\n",
       " 'shakhsiyat',\n",
       " 'Waheed',\n",
       " 'mei',\n",
       " 'Apni',\n",
       " 'class',\n",
       " 'martaba',\n",
       " 'Di',\n",
       " 'yaar',\n",
       " 'best',\n",
       " 'Bhutto',\n",
       " 'mubarak',\n",
       " 'kren',\n",
       " 'Muslim',\n",
       " 'farmaye',\n",
       " 'lagi',\n",
       " 'bas',\n",
       " 'Kay',\n",
       " 'faida',\n",
       " 'Bad',\n",
       " 'krti',\n",
       " 'Tu',\n",
       " 'sari',\n",
       " 'hee',\n",
       " 'thein',\n",
       " 'haal',\n",
       " 'bachi',\n",
       " 'gaey',\n",
       " 'unhein',\n",
       " 'General',\n",
       " 'hesiyat',\n",
       " 'waja',\n",
       " 'karain',\n",
       " 'August',\n",
       " 'Ghalib',\n",
       " 'Murad',\n",
       " 'Kuch',\n",
       " 'dene',\n",
       " 'ada',\n",
       " 'shareef',\n",
       " 'howae',\n",
       " 'liay',\n",
       " 'Band',\n",
       " 'krny',\n",
       " 'kamiyabi',\n",
       " 'jari',\n",
       " 'bohot',\n",
       " 'your',\n",
       " 'pic',\n",
       " 'ly',\n",
       " 'zaroor',\n",
       " 'wohi',\n",
       " 'rakhe',\n",
       " 'Karen',\n",
       " 'bc',\n",
       " 'tehreek',\n",
       " 'jana',\n",
       " 'pay',\n",
       " 'great',\n",
       " 'more',\n",
       " 'soch',\n",
       " 'Hahahaha',\n",
       " 'Lekin',\n",
       " 'Jan',\n",
       " 'election',\n",
       " 'Kal',\n",
       " 'krta',\n",
       " 'end',\n",
       " 'agr',\n",
       " 'mn',\n",
       " 'mery',\n",
       " 'isko',\n",
       " 'Gai',\n",
       " 'phone',\n",
       " 'bhar',\n",
       " 'banane',\n",
       " 'Beshak',\n",
       " 'Tak',\n",
       " 'beti',\n",
       " 'zabardast',\n",
       " 'tareekh',\n",
       " 'naya',\n",
       " 'Kisi',\n",
       " 'pur',\n",
       " 'Inho',\n",
       " 'episode',\n",
       " 'kharab',\n",
       " 'y',\n",
       " 'buhat',\n",
       " 'Bano',\n",
       " 'reh',\n",
       " 'satah',\n",
       " 'Bari',\n",
       " 'Great',\n",
       " 'uski',\n",
       " 'hogi',\n",
       " 'ba',\n",
       " 'asar',\n",
       " 'support',\n",
       " 'Diya',\n",
       " 'Saeed',\n",
       " 'rakhte',\n",
       " 'siyasat',\n",
       " '*',\n",
       " 'mili',\n",
       " 'zyada',\n",
       " 'pyar',\n",
       " 'with',\n",
       " 'r',\n",
       " 'nahin',\n",
       " 'ziada',\n",
       " 'deta',\n",
       " 'chahye',\n",
       " 'name',\n",
       " 'khelari',\n",
       " '7',\n",
       " 'school',\n",
       " 'Mujhe',\n",
       " 'kehna',\n",
       " 'chala',\n",
       " 'Tahum',\n",
       " 'moqa',\n",
       " 'Hoga',\n",
       " 'Quaid',\n",
       " 'Justice',\n",
       " 'nam',\n",
       " 'M',\n",
       " 'ati',\n",
       " 'hifazat',\n",
       " 'hony',\n",
       " 'roz',\n",
       " 'Phir',\n",
       " 'baqi',\n",
       " 'award',\n",
       " 'Masha',\n",
       " 'Raha',\n",
       " 'Altaf',\n",
       " 'hoty',\n",
       " 'career',\n",
       " 'faisla',\n",
       " 'janib',\n",
       " 'kahan',\n",
       " 'krne',\n",
       " 'Musharraf',\n",
       " 'G',\n",
       " 'Alam',\n",
       " 'deti',\n",
       " 't',\n",
       " 'please',\n",
       " 'Salman',\n",
       " 'para',\n",
       " 'Sadar',\n",
       " 'pehly',\n",
       " 'He',\n",
       " 'Akram',\n",
       " 'thora',\n",
       " 'mehnat',\n",
       " 'face',\n",
       " 'bahar',\n",
       " 'Chaudhry',\n",
       " 'pass',\n",
       " 'istemal',\n",
       " 'qabil',\n",
       " 'Yar',\n",
       " 'Gaya',\n",
       " 'Mohammad',\n",
       " 'khana',\n",
       " 'sake',\n",
       " 'Mohammed',\n",
       " 'theek',\n",
       " 'Zia',\n",
       " 'matches',\n",
       " 'qabool',\n",
       " 'Mn',\n",
       " 'barhe',\n",
       " 'esa',\n",
       " 'barha',\n",
       " 'March',\n",
       " 'art',\n",
       " 'thin',\n",
       " 'mujh',\n",
       " 'phele',\n",
       " 'v',\n",
       " 'geet',\n",
       " 'Very',\n",
       " 'karen',\n",
       " '12',\n",
       " 'December',\n",
       " '15',\n",
       " 'bacha',\n",
       " 'muslim',\n",
       " 'imran',\n",
       " 'hoye',\n",
       " 'Islam',\n",
       " 'baar',\n",
       " 'Sath',\n",
       " 'kch',\n",
       " 'doran',\n",
       " 'karkardagi',\n",
       " 'meray',\n",
       " 'dosri',\n",
       " 'lena',\n",
       " 'Aunty',\n",
       " 'world',\n",
       " 'dunia',\n",
       " 'unhe',\n",
       " 'order',\n",
       " 'Hafeez',\n",
       " 'ache',\n",
       " 'mout',\n",
       " 'watan',\n",
       " 'sahib',\n",
       " 'hamare',\n",
       " 'kae',\n",
       " 'Mai',\n",
       " 'mutabiq',\n",
       " 'daur',\n",
       " '11',\n",
       " 'who',\n",
       " 'PAK',\n",
       " '・',\n",
       " 'jay',\n",
       " 'tour',\n",
       " 'teen',\n",
       " 'maa',\n",
       " 'World',\n",
       " 'Haha',\n",
       " 'dafa',\n",
       " 'Karo',\n",
       " 'bt',\n",
       " 'bakwas',\n",
       " 'Bus',\n",
       " 'jao',\n",
       " 'andaza',\n",
       " 'mukhtalif',\n",
       " '14',\n",
       " 'pori',\n",
       " 'role',\n",
       " 'awaaz',\n",
       " 'unhain',\n",
       " 'O',\n",
       " 'Love',\n",
       " 'inki',\n",
       " 'Aj',\n",
       " 'esi',\n",
       " 'Dr',\n",
       " 'Lia',\n",
       " 'ghalat',\n",
       " 'check',\n",
       " '😜',\n",
       " 'Dil',\n",
       " 'sada',\n",
       " 'banaya',\n",
       " 'jesi',\n",
       " 'pora',\n",
       " 'men',\n",
       " 'kamal',\n",
       " 'Awam',\n",
       " '19',\n",
       " 'farmay',\n",
       " 'pagal',\n",
       " 'jiya',\n",
       " 'parha',\n",
       " 'Shakhs',\n",
       " 'chali',\n",
       " 'Han',\n",
       " 'bachay',\n",
       " 'title',\n",
       " 'set',\n",
       " 'sal',\n",
       " 'A',\n",
       " 'gayi',\n",
       " 'dunya',\n",
       " 'sun',\n",
       " '9',\n",
       " 'Asif',\n",
       " 'murshad',\n",
       " 'Nhi',\n",
       " 'Sab',\n",
       " 'kahani',\n",
       " 'dar',\n",
       " 'zinda',\n",
       " 'uss',\n",
       " 'Chief',\n",
       " 'Gaye',\n",
       " 'nice',\n",
       " 'Army',\n",
       " 'insaan',\n",
       " 'achy',\n",
       " 'Shaks',\n",
       " 'E',\n",
       " 'Judge',\n",
       " 'buri',\n",
       " 'next',\n",
       " 'new',\n",
       " 'Isi',\n",
       " 'November',\n",
       " '50',\n",
       " 'cup',\n",
       " 'beta',\n",
       " 'shohrat',\n",
       " 'not',\n",
       " 'frma',\n",
       " 'sehat',\n",
       " 'hazrat',\n",
       " 'Intikhab',\n",
       " 'tab',\n",
       " 'ik',\n",
       " 'gay',\n",
       " 'naseeb',\n",
       " 'ji',\n",
       " 'deen',\n",
       " '\\xa0',\n",
       " 'sahir',\n",
       " 'daal',\n",
       " 'dobara',\n",
       " 'khilariyon',\n",
       " 'January',\n",
       " 'Yaar',\n",
       " 'B',\n",
       " 'atta',\n",
       " 'yaqeen',\n",
       " 'uncle',\n",
       " 'saza',\n",
       " 'hey',\n",
       " 'gia',\n",
       " 'Har',\n",
       " 'shirts',\n",
       " 'Mery',\n",
       " 'Allama',\n",
       " 'Sarojini',\n",
       " 'khelaf',\n",
       " 'chuki',\n",
       " 'just',\n",
       " 'chukka',\n",
       " 'shikar',\n",
       " 'India',\n",
       " 'izzat',\n",
       " 'bary',\n",
       " 'dey',\n",
       " 'Mashallah',\n",
       " 'khatoon',\n",
       " '😘',\n",
       " 'Bijli',\n",
       " 'Sy',\n",
       " 'Gandhi',\n",
       " 'la',\n",
       " 'rahey',\n",
       " 'sach',\n",
       " 'Road',\n",
       " 'lagaya',\n",
       " 'lahore',\n",
       " 'Ke',\n",
       " 'deye',\n",
       " 'ese',\n",
       " 'PM',\n",
       " 'App',\n",
       " 'post',\n",
       " 'huay',\n",
       " 'KO',\n",
       " 'hoqoq',\n",
       " 'achay',\n",
       " 'hospital',\n",
       " 'sharam',\n",
       " 'hahaha',\n",
       " 'University',\n",
       " 'Murshid',\n",
       " 'yr',\n",
       " 'mard',\n",
       " 'Khilaf',\n",
       " 'shayad',\n",
       " 'shakal',\n",
       " 'Bhatti',\n",
       " 'Hindustan',\n",
       " 'saka',\n",
       " 'taraqi',\n",
       " 'dal',\n",
       " 'ay',\n",
       " 'maqboliyat',\n",
       " 'shows',\n",
       " 'bach',\n",
       " 'Wah',\n",
       " 'out',\n",
       " 'andaaz',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(wordDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMatrix = [] # 建立词条向量\n",
    "for review in data:\n",
    "    trainMatrix.append(setOfWords2Vec(wordDict[:13000], review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(trainMatrix, target, random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7509481668773704"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf = naive_bayes.BernoulliNB()\n",
    "nb_clf.fit(X_train, y_train)\n",
    "nb_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
